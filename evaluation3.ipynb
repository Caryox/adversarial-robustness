{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch                                  #Imports \n",
    "import torchvision\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class evaluation():   #Evaluation Class\n",
    "    \n",
    " def __init__(self,name ): \n",
    "     self.name=name\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chovek():                                   #Evaluation Class \n",
    "  def __init__(self,name ):                               # which parameters should it receive \n",
    "    \n",
    "   self.name = name\n",
    "\n",
    "  def hi(self):\n",
    "    \n",
    "    message = f'hi, az sum {self.name}'\n",
    "    return message\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hi, az sum Ivan'"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ivan = Chovek(name='Ivan')\n",
    "\n",
    "ivan.hi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(                             #Train the Data and Testloaders and normalize the inputs\n",
    "  torchvision.datasets.MNIST('/files/', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('/files/', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1d8b9aa58b0>"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_epochs = 3                       #Definition of Hyper Parameters\n",
    "batch_size_train = 60\n",
    "batch_size_test = 1000\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "log_interval = 10\n",
    "\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = enumerate(test_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 1, 28, 28])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt               # Display six images \n",
    "\n",
    "fig = plt.figure()\n",
    "for i in range(6):\n",
    "  plt.subplot(2,3,i+1)\n",
    "  plt.tight_layout()\n",
    "  plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
    "  plt.title(\"Ground Truth: {}\".format(example_targets[i]))\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn                            #Imports of Neural Network \n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):                                        #Neural Network\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Net()                                                                     #Initialization of Neural Network and of the Optimizer\n",
    "optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []                                             # We'll also keep track of the progress with some printouts. In order to create a nice training curve later on we also create two lists for saving training and testing losses. On the x-axis we want to display the number of training examples the network has seen during training. \n",
    "train_counter = []\n",
    "test_losses = []\n",
    "test_counter = [i*len(train_loader.dataset) for i in range(n_epochs + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def train(epoch):\n",
    "                              # Training of the Neural Network\n",
    "  network.train()\n",
    "  for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    optimizer.zero_grad()\n",
    "    output = network(data)\n",
    "    loss = F.nll_loss(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if batch_idx % log_interval == 0:\n",
    "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "        epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "        100. * batch_idx / len(train_loader), loss.item()))\n",
    "      train_losses.append(loss.item())\n",
    "      train_counter.append(\n",
    "        (batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))\n",
    "      torch.save(network.state_dict(), str(os.getcwd()) + '/results/model.pth')\n",
    "      torch.save(optimizer.state_dict(), str(os.getcwd()) + '/results/optimizer.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "  network.eval()\n",
    "  test_loss = 0\n",
    "  correct = 0\n",
    "  with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "      output = network(data)\n",
    "      test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "      pred = output.data.max(1, keepdim=True)[1]\n",
    "      correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "  test_loss /= len(test_loader.dataset)\n",
    "  test_losses.append(test_loss)\n",
    "  print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ivank\\AppData\\Local\\Temp/ipykernel_24104/2161508242.py:17: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 2.3096, Accuracy: 924/10000 (9%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.311668\n",
      "Train Epoch: 1 [600/60000 (1%)]\tLoss: 2.301118\n",
      "Train Epoch: 1 [1200/60000 (2%)]\tLoss: 2.288145\n",
      "Train Epoch: 1 [1800/60000 (3%)]\tLoss: 2.247380\n",
      "Train Epoch: 1 [2400/60000 (4%)]\tLoss: 2.253007\n",
      "Train Epoch: 1 [3000/60000 (5%)]\tLoss: 2.238811\n",
      "Train Epoch: 1 [3600/60000 (6%)]\tLoss: 2.226169\n",
      "Train Epoch: 1 [4200/60000 (7%)]\tLoss: 2.167308\n",
      "Train Epoch: 1 [4800/60000 (8%)]\tLoss: 2.093194\n",
      "Train Epoch: 1 [5400/60000 (9%)]\tLoss: 2.108451\n",
      "Train Epoch: 1 [6000/60000 (10%)]\tLoss: 1.913602\n",
      "Train Epoch: 1 [6600/60000 (11%)]\tLoss: 1.685405\n",
      "Train Epoch: 1 [7200/60000 (12%)]\tLoss: 1.731126\n",
      "Train Epoch: 1 [7800/60000 (13%)]\tLoss: 1.769907\n",
      "Train Epoch: 1 [8400/60000 (14%)]\tLoss: 1.515585\n",
      "Train Epoch: 1 [9000/60000 (15%)]\tLoss: 1.493429\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 1.193665\n",
      "Train Epoch: 1 [10200/60000 (17%)]\tLoss: 1.502354\n",
      "Train Epoch: 1 [10800/60000 (18%)]\tLoss: 0.964918\n",
      "Train Epoch: 1 [11400/60000 (19%)]\tLoss: 1.101748\n",
      "Train Epoch: 1 [12000/60000 (20%)]\tLoss: 1.032535\n",
      "Train Epoch: 1 [12600/60000 (21%)]\tLoss: 1.064307\n",
      "Train Epoch: 1 [13200/60000 (22%)]\tLoss: 1.084430\n",
      "Train Epoch: 1 [13800/60000 (23%)]\tLoss: 1.053845\n",
      "Train Epoch: 1 [14400/60000 (24%)]\tLoss: 0.971799\n",
      "Train Epoch: 1 [15000/60000 (25%)]\tLoss: 0.894735\n",
      "Train Epoch: 1 [15600/60000 (26%)]\tLoss: 0.903686\n",
      "Train Epoch: 1 [16200/60000 (27%)]\tLoss: 0.752942\n",
      "Train Epoch: 1 [16800/60000 (28%)]\tLoss: 0.863446\n",
      "Train Epoch: 1 [17400/60000 (29%)]\tLoss: 0.780469\n",
      "Train Epoch: 1 [18000/60000 (30%)]\tLoss: 1.056310\n",
      "Train Epoch: 1 [18600/60000 (31%)]\tLoss: 0.730089\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.987829\n",
      "Train Epoch: 1 [19800/60000 (33%)]\tLoss: 0.680032\n",
      "Train Epoch: 1 [20400/60000 (34%)]\tLoss: 0.730195\n",
      "Train Epoch: 1 [21000/60000 (35%)]\tLoss: 0.726130\n",
      "Train Epoch: 1 [21600/60000 (36%)]\tLoss: 0.859607\n",
      "Train Epoch: 1 [22200/60000 (37%)]\tLoss: 0.886076\n",
      "Train Epoch: 1 [22800/60000 (38%)]\tLoss: 0.698850\n",
      "Train Epoch: 1 [23400/60000 (39%)]\tLoss: 0.737374\n",
      "Train Epoch: 1 [24000/60000 (40%)]\tLoss: 0.670288\n",
      "Train Epoch: 1 [24600/60000 (41%)]\tLoss: 0.865339\n",
      "Train Epoch: 1 [25200/60000 (42%)]\tLoss: 0.596775\n",
      "Train Epoch: 1 [25800/60000 (43%)]\tLoss: 0.864814\n",
      "Train Epoch: 1 [26400/60000 (44%)]\tLoss: 0.542867\n",
      "Train Epoch: 1 [27000/60000 (45%)]\tLoss: 0.579256\n",
      "Train Epoch: 1 [27600/60000 (46%)]\tLoss: 0.610840\n",
      "Train Epoch: 1 [28200/60000 (47%)]\tLoss: 0.686982\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.372334\n",
      "Train Epoch: 1 [29400/60000 (49%)]\tLoss: 0.649945\n",
      "Train Epoch: 1 [30000/60000 (50%)]\tLoss: 0.712073\n",
      "Train Epoch: 1 [30600/60000 (51%)]\tLoss: 0.471445\n",
      "Train Epoch: 1 [31200/60000 (52%)]\tLoss: 0.650926\n",
      "Train Epoch: 1 [31800/60000 (53%)]\tLoss: 0.888146\n",
      "Train Epoch: 1 [32400/60000 (54%)]\tLoss: 0.550394\n",
      "Train Epoch: 1 [33000/60000 (55%)]\tLoss: 0.572197\n",
      "Train Epoch: 1 [33600/60000 (56%)]\tLoss: 0.638142\n",
      "Train Epoch: 1 [34200/60000 (57%)]\tLoss: 0.603684\n",
      "Train Epoch: 1 [34800/60000 (58%)]\tLoss: 0.415305\n",
      "Train Epoch: 1 [35400/60000 (59%)]\tLoss: 0.645436\n",
      "Train Epoch: 1 [36000/60000 (60%)]\tLoss: 0.531295\n",
      "Train Epoch: 1 [36600/60000 (61%)]\tLoss: 0.630457\n",
      "Train Epoch: 1 [37200/60000 (62%)]\tLoss: 0.480224\n",
      "Train Epoch: 1 [37800/60000 (63%)]\tLoss: 0.664959\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.579162\n",
      "Train Epoch: 1 [39000/60000 (65%)]\tLoss: 0.516937\n",
      "Train Epoch: 1 [39600/60000 (66%)]\tLoss: 0.293784\n",
      "Train Epoch: 1 [40200/60000 (67%)]\tLoss: 0.679059\n",
      "Train Epoch: 1 [40800/60000 (68%)]\tLoss: 0.253416\n",
      "Train Epoch: 1 [41400/60000 (69%)]\tLoss: 0.586887\n",
      "Train Epoch: 1 [42000/60000 (70%)]\tLoss: 0.514555\n",
      "Train Epoch: 1 [42600/60000 (71%)]\tLoss: 0.590077\n",
      "Train Epoch: 1 [43200/60000 (72%)]\tLoss: 0.567257\n",
      "Train Epoch: 1 [43800/60000 (73%)]\tLoss: 0.534062\n",
      "Train Epoch: 1 [44400/60000 (74%)]\tLoss: 0.422319\n",
      "Train Epoch: 1 [45000/60000 (75%)]\tLoss: 0.524717\n",
      "Train Epoch: 1 [45600/60000 (76%)]\tLoss: 0.431496\n",
      "Train Epoch: 1 [46200/60000 (77%)]\tLoss: 0.623155\n",
      "Train Epoch: 1 [46800/60000 (78%)]\tLoss: 0.527242\n",
      "Train Epoch: 1 [47400/60000 (79%)]\tLoss: 0.425134\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.539091\n",
      "Train Epoch: 1 [48600/60000 (81%)]\tLoss: 0.617586\n",
      "Train Epoch: 1 [49200/60000 (82%)]\tLoss: 0.612667\n",
      "Train Epoch: 1 [49800/60000 (83%)]\tLoss: 0.457993\n",
      "Train Epoch: 1 [50400/60000 (84%)]\tLoss: 0.349197\n",
      "Train Epoch: 1 [51000/60000 (85%)]\tLoss: 0.376001\n",
      "Train Epoch: 1 [51600/60000 (86%)]\tLoss: 0.370147\n",
      "Train Epoch: 1 [52200/60000 (87%)]\tLoss: 0.501052\n",
      "Train Epoch: 1 [52800/60000 (88%)]\tLoss: 0.582268\n",
      "Train Epoch: 1 [53400/60000 (89%)]\tLoss: 0.371595\n",
      "Train Epoch: 1 [54000/60000 (90%)]\tLoss: 0.326335\n",
      "Train Epoch: 1 [54600/60000 (91%)]\tLoss: 0.393756\n",
      "Train Epoch: 1 [55200/60000 (92%)]\tLoss: 0.432467\n",
      "Train Epoch: 1 [55800/60000 (93%)]\tLoss: 0.949432\n",
      "Train Epoch: 1 [56400/60000 (94%)]\tLoss: 0.223816\n",
      "Train Epoch: 1 [57000/60000 (95%)]\tLoss: 0.515807\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.418052\n",
      "Train Epoch: 1 [58200/60000 (97%)]\tLoss: 0.292915\n",
      "Train Epoch: 1 [58800/60000 (98%)]\tLoss: 0.336545\n",
      "Train Epoch: 1 [59400/60000 (99%)]\tLoss: 0.585969\n",
      "\n",
      "Test set: Avg. loss: 0.1855, Accuracy: 9444/10000 (94%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.411328\n",
      "Train Epoch: 2 [600/60000 (1%)]\tLoss: 0.614628\n",
      "Train Epoch: 2 [1200/60000 (2%)]\tLoss: 0.489572\n",
      "Train Epoch: 2 [1800/60000 (3%)]\tLoss: 0.587751\n",
      "Train Epoch: 2 [2400/60000 (4%)]\tLoss: 0.832469\n",
      "Train Epoch: 2 [3000/60000 (5%)]\tLoss: 0.414054\n",
      "Train Epoch: 2 [3600/60000 (6%)]\tLoss: 0.153644\n",
      "Train Epoch: 2 [4200/60000 (7%)]\tLoss: 0.612559\n",
      "Train Epoch: 2 [4800/60000 (8%)]\tLoss: 0.273381\n",
      "Train Epoch: 2 [5400/60000 (9%)]\tLoss: 0.450707\n",
      "Train Epoch: 2 [6000/60000 (10%)]\tLoss: 0.497277\n",
      "Train Epoch: 2 [6600/60000 (11%)]\tLoss: 0.456778\n",
      "Train Epoch: 2 [7200/60000 (12%)]\tLoss: 0.374505\n",
      "Train Epoch: 2 [7800/60000 (13%)]\tLoss: 0.360054\n",
      "Train Epoch: 2 [8400/60000 (14%)]\tLoss: 0.446022\n",
      "Train Epoch: 2 [9000/60000 (15%)]\tLoss: 0.727664\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.245776\n",
      "Train Epoch: 2 [10200/60000 (17%)]\tLoss: 0.273963\n",
      "Train Epoch: 2 [10800/60000 (18%)]\tLoss: 0.489578\n",
      "Train Epoch: 2 [11400/60000 (19%)]\tLoss: 0.243087\n",
      "Train Epoch: 2 [12000/60000 (20%)]\tLoss: 0.404666\n",
      "Train Epoch: 2 [12600/60000 (21%)]\tLoss: 0.307933\n",
      "Train Epoch: 2 [13200/60000 (22%)]\tLoss: 0.442791\n",
      "Train Epoch: 2 [13800/60000 (23%)]\tLoss: 0.326097\n",
      "Train Epoch: 2 [14400/60000 (24%)]\tLoss: 0.291680\n",
      "Train Epoch: 2 [15000/60000 (25%)]\tLoss: 0.325859\n",
      "Train Epoch: 2 [15600/60000 (26%)]\tLoss: 0.523184\n",
      "Train Epoch: 2 [16200/60000 (27%)]\tLoss: 0.329621\n",
      "Train Epoch: 2 [16800/60000 (28%)]\tLoss: 0.426948\n",
      "Train Epoch: 2 [17400/60000 (29%)]\tLoss: 0.524266\n",
      "Train Epoch: 2 [18000/60000 (30%)]\tLoss: 0.268215\n",
      "Train Epoch: 2 [18600/60000 (31%)]\tLoss: 0.378468\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.651517\n",
      "Train Epoch: 2 [19800/60000 (33%)]\tLoss: 0.487918\n",
      "Train Epoch: 2 [20400/60000 (34%)]\tLoss: 0.385431\n",
      "Train Epoch: 2 [21000/60000 (35%)]\tLoss: 0.237887\n",
      "Train Epoch: 2 [21600/60000 (36%)]\tLoss: 0.324987\n",
      "Train Epoch: 2 [22200/60000 (37%)]\tLoss: 0.786926\n",
      "Train Epoch: 2 [22800/60000 (38%)]\tLoss: 0.555153\n",
      "Train Epoch: 2 [23400/60000 (39%)]\tLoss: 0.428499\n",
      "Train Epoch: 2 [24000/60000 (40%)]\tLoss: 0.450406\n",
      "Train Epoch: 2 [24600/60000 (41%)]\tLoss: 0.226092\n",
      "Train Epoch: 2 [25200/60000 (42%)]\tLoss: 0.327350\n",
      "Train Epoch: 2 [25800/60000 (43%)]\tLoss: 0.432003\n",
      "Train Epoch: 2 [26400/60000 (44%)]\tLoss: 0.407365\n",
      "Train Epoch: 2 [27000/60000 (45%)]\tLoss: 0.290214\n",
      "Train Epoch: 2 [27600/60000 (46%)]\tLoss: 0.430027\n",
      "Train Epoch: 2 [28200/60000 (47%)]\tLoss: 0.245591\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.367341\n",
      "Train Epoch: 2 [29400/60000 (49%)]\tLoss: 0.351730\n",
      "Train Epoch: 2 [30000/60000 (50%)]\tLoss: 0.317501\n",
      "Train Epoch: 2 [30600/60000 (51%)]\tLoss: 0.238393\n",
      "Train Epoch: 2 [31200/60000 (52%)]\tLoss: 0.231795\n",
      "Train Epoch: 2 [31800/60000 (53%)]\tLoss: 0.347244\n",
      "Train Epoch: 2 [32400/60000 (54%)]\tLoss: 0.377326\n",
      "Train Epoch: 2 [33000/60000 (55%)]\tLoss: 0.539056\n",
      "Train Epoch: 2 [33600/60000 (56%)]\tLoss: 0.372145\n",
      "Train Epoch: 2 [34200/60000 (57%)]\tLoss: 0.304234\n",
      "Train Epoch: 2 [34800/60000 (58%)]\tLoss: 0.499244\n",
      "Train Epoch: 2 [35400/60000 (59%)]\tLoss: 0.377315\n",
      "Train Epoch: 2 [36000/60000 (60%)]\tLoss: 0.317302\n",
      "Train Epoch: 2 [36600/60000 (61%)]\tLoss: 0.207858\n",
      "Train Epoch: 2 [37200/60000 (62%)]\tLoss: 0.242801\n",
      "Train Epoch: 2 [37800/60000 (63%)]\tLoss: 0.342138\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.307958\n",
      "Train Epoch: 2 [39000/60000 (65%)]\tLoss: 0.457618\n",
      "Train Epoch: 2 [39600/60000 (66%)]\tLoss: 0.399607\n",
      "Train Epoch: 2 [40200/60000 (67%)]\tLoss: 0.299604\n",
      "Train Epoch: 2 [40800/60000 (68%)]\tLoss: 0.467290\n",
      "Train Epoch: 2 [41400/60000 (69%)]\tLoss: 0.450586\n",
      "Train Epoch: 2 [42000/60000 (70%)]\tLoss: 0.294414\n",
      "Train Epoch: 2 [42600/60000 (71%)]\tLoss: 0.198373\n",
      "Train Epoch: 2 [43200/60000 (72%)]\tLoss: 0.245525\n",
      "Train Epoch: 2 [43800/60000 (73%)]\tLoss: 0.508808\n",
      "Train Epoch: 2 [44400/60000 (74%)]\tLoss: 0.360477\n",
      "Train Epoch: 2 [45000/60000 (75%)]\tLoss: 0.414053\n",
      "Train Epoch: 2 [45600/60000 (76%)]\tLoss: 0.477001\n",
      "Train Epoch: 2 [46200/60000 (77%)]\tLoss: 0.214900\n",
      "Train Epoch: 2 [46800/60000 (78%)]\tLoss: 0.286911\n",
      "Train Epoch: 2 [47400/60000 (79%)]\tLoss: 0.507894\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.460063\n",
      "Train Epoch: 2 [48600/60000 (81%)]\tLoss: 0.457616\n",
      "Train Epoch: 2 [49200/60000 (82%)]\tLoss: 0.313267\n",
      "Train Epoch: 2 [49800/60000 (83%)]\tLoss: 0.191426\n",
      "Train Epoch: 2 [50400/60000 (84%)]\tLoss: 0.213160\n",
      "Train Epoch: 2 [51000/60000 (85%)]\tLoss: 0.305717\n",
      "Train Epoch: 2 [51600/60000 (86%)]\tLoss: 0.245901\n",
      "Train Epoch: 2 [52200/60000 (87%)]\tLoss: 0.239925\n",
      "Train Epoch: 2 [52800/60000 (88%)]\tLoss: 0.269168\n",
      "Train Epoch: 2 [53400/60000 (89%)]\tLoss: 0.447707\n",
      "Train Epoch: 2 [54000/60000 (90%)]\tLoss: 0.739809\n",
      "Train Epoch: 2 [54600/60000 (91%)]\tLoss: 0.687728\n",
      "Train Epoch: 2 [55200/60000 (92%)]\tLoss: 0.473758\n",
      "Train Epoch: 2 [55800/60000 (93%)]\tLoss: 0.314516\n",
      "Train Epoch: 2 [56400/60000 (94%)]\tLoss: 0.229968\n",
      "Train Epoch: 2 [57000/60000 (95%)]\tLoss: 0.238783\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.279349\n",
      "Train Epoch: 2 [58200/60000 (97%)]\tLoss: 0.643481\n",
      "Train Epoch: 2 [58800/60000 (98%)]\tLoss: 0.537546\n",
      "Train Epoch: 2 [59400/60000 (99%)]\tLoss: 0.132342\n",
      "\n",
      "Test set: Avg. loss: 0.1165, Accuracy: 9645/10000 (96%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.229211\n",
      "Train Epoch: 3 [600/60000 (1%)]\tLoss: 0.414913\n",
      "Train Epoch: 3 [1200/60000 (2%)]\tLoss: 0.205214\n",
      "Train Epoch: 3 [1800/60000 (3%)]\tLoss: 0.185933\n",
      "Train Epoch: 3 [2400/60000 (4%)]\tLoss: 0.370286\n",
      "Train Epoch: 3 [3000/60000 (5%)]\tLoss: 0.458593\n",
      "Train Epoch: 3 [3600/60000 (6%)]\tLoss: 0.228744\n",
      "Train Epoch: 3 [4200/60000 (7%)]\tLoss: 0.277087\n",
      "Train Epoch: 3 [4800/60000 (8%)]\tLoss: 0.292186\n",
      "Train Epoch: 3 [5400/60000 (9%)]\tLoss: 0.478157\n",
      "Train Epoch: 3 [6000/60000 (10%)]\tLoss: 0.408867\n",
      "Train Epoch: 3 [6600/60000 (11%)]\tLoss: 0.271975\n",
      "Train Epoch: 3 [7200/60000 (12%)]\tLoss: 0.199157\n",
      "Train Epoch: 3 [7800/60000 (13%)]\tLoss: 0.407070\n",
      "Train Epoch: 3 [8400/60000 (14%)]\tLoss: 0.500001\n",
      "Train Epoch: 3 [9000/60000 (15%)]\tLoss: 0.238110\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.191110\n",
      "Train Epoch: 3 [10200/60000 (17%)]\tLoss: 0.450181\n",
      "Train Epoch: 3 [10800/60000 (18%)]\tLoss: 0.172973\n",
      "Train Epoch: 3 [11400/60000 (19%)]\tLoss: 0.350916\n",
      "Train Epoch: 3 [12000/60000 (20%)]\tLoss: 0.452220\n",
      "Train Epoch: 3 [12600/60000 (21%)]\tLoss: 0.329613\n",
      "Train Epoch: 3 [13200/60000 (22%)]\tLoss: 0.168253\n",
      "Train Epoch: 3 [13800/60000 (23%)]\tLoss: 0.476720\n",
      "Train Epoch: 3 [14400/60000 (24%)]\tLoss: 0.550418\n",
      "Train Epoch: 3 [15000/60000 (25%)]\tLoss: 0.323335\n",
      "Train Epoch: 3 [15600/60000 (26%)]\tLoss: 0.179227\n",
      "Train Epoch: 3 [16200/60000 (27%)]\tLoss: 0.219811\n",
      "Train Epoch: 3 [16800/60000 (28%)]\tLoss: 0.460276\n",
      "Train Epoch: 3 [17400/60000 (29%)]\tLoss: 0.207447\n",
      "Train Epoch: 3 [18000/60000 (30%)]\tLoss: 0.292638\n",
      "Train Epoch: 3 [18600/60000 (31%)]\tLoss: 0.356656\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.400238\n",
      "Train Epoch: 3 [19800/60000 (33%)]\tLoss: 0.333626\n",
      "Train Epoch: 3 [20400/60000 (34%)]\tLoss: 0.368656\n",
      "Train Epoch: 3 [21000/60000 (35%)]\tLoss: 0.246046\n",
      "Train Epoch: 3 [21600/60000 (36%)]\tLoss: 0.317570\n",
      "Train Epoch: 3 [22200/60000 (37%)]\tLoss: 0.368667\n",
      "Train Epoch: 3 [22800/60000 (38%)]\tLoss: 0.472958\n",
      "Train Epoch: 3 [23400/60000 (39%)]\tLoss: 0.462502\n",
      "Train Epoch: 3 [24000/60000 (40%)]\tLoss: 0.317813\n",
      "Train Epoch: 3 [24600/60000 (41%)]\tLoss: 0.383446\n",
      "Train Epoch: 3 [25200/60000 (42%)]\tLoss: 0.197691\n",
      "Train Epoch: 3 [25800/60000 (43%)]\tLoss: 0.226091\n",
      "Train Epoch: 3 [26400/60000 (44%)]\tLoss: 0.258193\n",
      "Train Epoch: 3 [27000/60000 (45%)]\tLoss: 0.401681\n",
      "Train Epoch: 3 [27600/60000 (46%)]\tLoss: 0.246844\n",
      "Train Epoch: 3 [28200/60000 (47%)]\tLoss: 0.338503\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.360317\n",
      "Train Epoch: 3 [29400/60000 (49%)]\tLoss: 0.220775\n",
      "Train Epoch: 3 [30000/60000 (50%)]\tLoss: 0.265447\n",
      "Train Epoch: 3 [30600/60000 (51%)]\tLoss: 0.255880\n",
      "Train Epoch: 3 [31200/60000 (52%)]\tLoss: 0.338004\n",
      "Train Epoch: 3 [31800/60000 (53%)]\tLoss: 0.165945\n",
      "Train Epoch: 3 [32400/60000 (54%)]\tLoss: 0.284317\n",
      "Train Epoch: 3 [33000/60000 (55%)]\tLoss: 0.203863\n",
      "Train Epoch: 3 [33600/60000 (56%)]\tLoss: 0.295599\n",
      "Train Epoch: 3 [34200/60000 (57%)]\tLoss: 0.150773\n",
      "Train Epoch: 3 [34800/60000 (58%)]\tLoss: 0.192792\n",
      "Train Epoch: 3 [35400/60000 (59%)]\tLoss: 0.467396\n",
      "Train Epoch: 3 [36000/60000 (60%)]\tLoss: 0.183161\n",
      "Train Epoch: 3 [36600/60000 (61%)]\tLoss: 0.277176\n",
      "Train Epoch: 3 [37200/60000 (62%)]\tLoss: 0.210911\n",
      "Train Epoch: 3 [37800/60000 (63%)]\tLoss: 0.210749\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.368916\n",
      "Train Epoch: 3 [39000/60000 (65%)]\tLoss: 0.402011\n",
      "Train Epoch: 3 [39600/60000 (66%)]\tLoss: 0.322027\n",
      "Train Epoch: 3 [40200/60000 (67%)]\tLoss: 0.150149\n",
      "Train Epoch: 3 [40800/60000 (68%)]\tLoss: 0.208879\n",
      "Train Epoch: 3 [41400/60000 (69%)]\tLoss: 0.195073\n",
      "Train Epoch: 3 [42000/60000 (70%)]\tLoss: 0.273225\n",
      "Train Epoch: 3 [42600/60000 (71%)]\tLoss: 0.248598\n",
      "Train Epoch: 3 [43200/60000 (72%)]\tLoss: 0.511991\n",
      "Train Epoch: 3 [43800/60000 (73%)]\tLoss: 0.280077\n",
      "Train Epoch: 3 [44400/60000 (74%)]\tLoss: 0.139768\n",
      "Train Epoch: 3 [45000/60000 (75%)]\tLoss: 0.336214\n",
      "Train Epoch: 3 [45600/60000 (76%)]\tLoss: 0.399994\n",
      "Train Epoch: 3 [46200/60000 (77%)]\tLoss: 0.220205\n",
      "Train Epoch: 3 [46800/60000 (78%)]\tLoss: 0.353531\n",
      "Train Epoch: 3 [47400/60000 (79%)]\tLoss: 0.207786\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.283633\n",
      "Train Epoch: 3 [48600/60000 (81%)]\tLoss: 0.284363\n",
      "Train Epoch: 3 [49200/60000 (82%)]\tLoss: 0.344330\n",
      "Train Epoch: 3 [49800/60000 (83%)]\tLoss: 0.469994\n",
      "Train Epoch: 3 [50400/60000 (84%)]\tLoss: 0.188004\n",
      "Train Epoch: 3 [51000/60000 (85%)]\tLoss: 0.378745\n",
      "Train Epoch: 3 [51600/60000 (86%)]\tLoss: 0.312056\n",
      "Train Epoch: 3 [52200/60000 (87%)]\tLoss: 0.198979\n",
      "Train Epoch: 3 [52800/60000 (88%)]\tLoss: 0.379221\n",
      "Train Epoch: 3 [53400/60000 (89%)]\tLoss: 0.317462\n",
      "Train Epoch: 3 [54000/60000 (90%)]\tLoss: 0.222033\n",
      "Train Epoch: 3 [54600/60000 (91%)]\tLoss: 0.346957\n",
      "Train Epoch: 3 [55200/60000 (92%)]\tLoss: 0.326617\n",
      "Train Epoch: 3 [55800/60000 (93%)]\tLoss: 0.280837\n",
      "Train Epoch: 3 [56400/60000 (94%)]\tLoss: 0.244440\n",
      "Train Epoch: 3 [57000/60000 (95%)]\tLoss: 0.379673\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.267756\n",
      "Train Epoch: 3 [58200/60000 (97%)]\tLoss: 0.109386\n",
      "Train Epoch: 3 [58800/60000 (98%)]\tLoss: 0.244652\n",
      "Train Epoch: 3 [59400/60000 (99%)]\tLoss: 0.447251\n",
      "\n",
      "Test set: Avg. loss: 0.0954, Accuracy: 9701/10000 (97%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test()\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "  train(epoch)\n",
    "  test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_counter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15952/3146178017.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_counter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_losses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'blue'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_counter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_losses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'red'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Train Loss'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Test Loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'upper right'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'number of training examples seen'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_counter' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(train_counter, train_losses, color='blue')\n",
    "plt.scatter(test_counter, test_losses, color='red')\n",
    "plt.legend(['Train Loss', 'Test Loss'], loc='upper right')\n",
    "plt.xlabel('number of training examples seen')\n",
    "plt.ylabel('negative log likelihood loss')\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_preds(model, loader):\n",
    "    all_preds = torch.tensor([])\n",
    "    for batch in loader:\n",
    "        images, labels = batch\n",
    "\n",
    "        preds = model(images)\n",
    "        all_preds = torch.cat(\n",
    "            (all_preds, preds)\n",
    "            ,dim=0\n",
    "        )\n",
    "    return all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ivank\\AppData\\Local\\Temp/ipykernel_24104/2161508242.py:17: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    prediction_loader = torch.utils.data.DataLoader(                             #Train the Data and Testloaders and normalize the inputs\n",
    "  torchvision.datasets.MNIST('/files/', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])))\n",
    "    train_preds = get_all_preds(network, prediction_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.0408e+01, -1.3709e+01, -1.6188e+01,  ..., -1.7318e+01,\n",
      "         -1.0667e+01, -9.2006e+00],\n",
      "        [-2.0146e-05, -1.8660e+01, -1.1309e+01,  ..., -1.6409e+01,\n",
      "         -1.4475e+01, -1.4082e+01],\n",
      "        [-1.8827e+01, -1.6302e+01, -1.2423e+01,  ..., -1.2560e+01,\n",
      "         -1.2810e+01, -3.8339e+00],\n",
      "        ...,\n",
      "        [-1.4277e+01, -2.0713e+01, -2.5359e+01,  ..., -2.1848e+01,\n",
      "         -1.1188e+01, -8.7156e+00],\n",
      "        [-7.2641e+00, -1.1322e+01, -9.4318e+00,  ..., -1.7674e+01,\n",
      "         -9.4335e+00, -1.2322e+01],\n",
      "        [-5.6276e+00, -9.3212e+00, -7.4222e+00,  ..., -7.9902e+00,\n",
      "         -2.2738e-02, -4.2860e+00]])\n"
     ]
    }
   ],
   "source": [
    "print(train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_correct(preds, labels):\n",
    "    return preds.argmax(dim=1).eq(labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataLoader' object has no attribute 'targets'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24104/913971530.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpreds_correct\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_num_correct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_preds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'total correct:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreds_correct\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'accuracy:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreds_correct\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataLoader' object has no attribute 'targets'"
     ]
    }
   ],
   "source": [
    "preds_correct = get_num_correct(train_preds, train_loader.)\n",
    "\n",
    "print('total correct:', preds_correct)\n",
    "print('accuracy:', preds_correct / len(train_loader))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6ba9f0aaac5c96417f7fc0d5c0dd45bd2bb51b270333297d0af638280622d206"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
